{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_LSTM_PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D-3mE4Nue1dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string \n",
        "import os\n",
        "import re\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aRsO6IiQe1d4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import GloVe Embedding as Dictionary"
      ]
    },
    {
      "metadata": {
        "id": "OFz8UfwNe1d7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dict = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coef = np.asarray(values[1:],dtype='float32')\n",
        "    embedding_dict[word] = coef\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQHpQdoNe1eB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(file):\n",
        "    processed = []\n",
        "    lines = file.read().split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.lower()\n",
        "        line = line.replace('1',' one ')\n",
        "        line = line.replace('2',' two ')\n",
        "        line = line.replace('3',' three ')\n",
        "        line = line.replace('4',' four ')\n",
        "        line = line.replace('5',' five ')\n",
        "        line = line.replace('6',' six ')\n",
        "        line = line.replace('7',' seven ')\n",
        "        line = line.replace('8',' eight ')\n",
        "        line = line.replace('9',' nine ')\n",
        "        line = line.replace('0',' zero ')\n",
        "        line = line.replace('#','<num>')\n",
        "        line = line.replace('-', ' ')\n",
        "        line = line.replace(\"'s'\", ' ')\n",
        "        line = re.sub(\" . \",' point ',line)\n",
        "        line = re.sub(\">.<\",' point ',line)\n",
        "        text = re.sub(r\"[^a-zA-Z?.!,'<>]+\",\" \",line)\n",
        "        text = text.rstrip().strip()\n",
        "        text = 'bos ' + text + ' eos'\n",
        "        processed.append(text)\n",
        "    return processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L39HXLPXe1eH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pf_train_in = os.getcwd()+'/train/train.article.txt'\n",
        "pf_train_sum = os.getcwd()+'/train/train.title.txt'\n",
        "text_train_in = open(pf_train_in,'r')\n",
        "text_train_sum = open(pf_train_sum,'r')\n",
        "\n",
        "lines_train_in = preprocess(text_train_in)\n",
        "lines_train_sum = preprocess(text_train_sum)\n",
        "\n",
        "TRAIN = pd.DataFrame({'Input':lines_train_in,'Sum1':lines_train_sum})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Noemuh62e1eL",
        "colab_type": "code",
        "outputId": "2cbe4d0c-34be-4d9a-9cd1-87d75341a36c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Sum1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;bos&gt; australia 's current account deficit shr...</td>\n",
              "      <td>&lt;bos&gt; australian current account deficit narro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;bos&gt; at least two people were killed in point...</td>\n",
              "      <td>&lt;bos&gt; at least two dead in southern philippine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;bos&gt; australian shares closed down &lt;num point...</td>\n",
              "      <td>&lt;bos&gt; australian stocks close down &lt;num point ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;bos&gt; south korea 's nuclear envoy kim sook ur...</td>\n",
              "      <td>&lt;bos&gt; envoy urges north korea to restart nucle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;bos&gt; south korea on monday announced sweeping...</td>\n",
              "      <td>&lt;bos&gt; skorea announces tax cuts to stimulate e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Input  \\\n",
              "0  <bos> australia 's current account deficit shr...   \n",
              "1  <bos> at least two people were killed in point...   \n",
              "2  <bos> australian shares closed down <num point...   \n",
              "3  <bos> south korea 's nuclear envoy kim sook ur...   \n",
              "4  <bos> south korea on monday announced sweeping...   \n",
              "\n",
              "                                                Sum1  \n",
              "0  <bos> australian current account deficit narro...  \n",
              "1  <bos> at least two dead in southern philippine...  \n",
              "2  <bos> australian stocks close down <num point ...  \n",
              "3  <bos> envoy urges north korea to restart nucle...  \n",
              "4  <bos> skorea announces tax cuts to stimulate e...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "EyiBz9xKe1eV",
        "colab_type": "code",
        "outputId": "e6d2a1b6-0096-4dee-cd70-5d0ca58422b8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"TRAIN has {} articles\".format(TRAIN.shape[0])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN has 3803958 articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3y3mR0tOe1ea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_in_len = []\n",
        "TRAIN_sum_len = [] \n",
        "\n",
        "for line in lines_train_in:\n",
        "    w = 0\n",
        "    for word in line.split(' '):\n",
        "        w += 1\n",
        "    TRAIN_in_len.append(w)\n",
        "    \n",
        "for line in lines_train_sum:\n",
        "    w = 0\n",
        "    for word in line.split(' '):\n",
        "        w += 1\n",
        "    TRAIN_sum_len.append(w)\n",
        "TRAIN_in_len = np.array(TRAIN_in_len)\n",
        "TRAIN_sum_len = np.array(TRAIN_sum_len) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCAA8hYIe1ee",
        "colab_type": "code",
        "outputId": "6f2f8a82-9ac3-4f2b-8d9a-92dab45a7c78",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"\\t\\t Min\\t 25%\\t Avg\\t 75%\\t Max\")\n",
        "print(\"\\nTRAIN Input\\t {}\\t{}\\t{}\\t{}\\t{}\".format(np.percentile(TRAIN_in_len,0),np.percentile(TRAIN_in_len,25),np.percentile(TRAIN_in_len,50),np.percentile(TRAIN_in_len,75),np.percentile(TRAIN_in_len,100)))\n",
        "print(\"TRAIN Summary\\t {}\\t{}\\t{}\\t{}\\t{}\".format(np.percentile(TRAIN_sum_len,0),np.percentile(TRAIN_sum_len,25),np.percentile(TRAIN_sum_len,50),np.percentile(TRAIN_sum_len,75),np.percentile(TRAIN_sum_len,100)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t Min\t 25%\t Avg\t 75%\t Max\n",
            "\n",
            "TRAIN Input\t 3.0\t29.0\t34.0\t39.0\t141.0\n",
            "TRAIN Summary\t 3.0\t9.0\t10.0\t12.0\t76.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oXh-KU-ze1eo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines_train_in = TRAIN.iloc[:,0].values\n",
        "lines_train_sum = TRAIN.iloc[:,1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p5xTZfM_e1eu",
        "colab_type": "code",
        "outputId": "0e9214f3-f468-4001-de60-562dd69468ed",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(TRAIN_in_len,bins=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.00000e+00, 0.00000e+00, 7.00000e+00, 2.30930e+04, 3.30900e+04,\n",
              "        8.08750e+04, 1.44740e+05, 2.51581e+05, 2.46434e+05, 4.77185e+05,\n",
              "        5.83727e+05, 6.06062e+05, 3.47907e+05, 3.90294e+05, 2.52407e+05,\n",
              "        1.55525e+05, 6.65320e+04, 6.32320e+04, 3.57770e+04, 2.03990e+04,\n",
              "        8.11000e+03, 7.49800e+03, 4.08400e+03, 2.25300e+03, 9.18000e+02,\n",
              "        8.57000e+02, 4.89000e+02, 3.38000e+02, 1.84000e+02, 8.80000e+01,\n",
              "        9.30000e+01, 6.50000e+01, 4.40000e+01, 2.10000e+01, 2.70000e+01,\n",
              "        1.60000e+01, 3.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
              "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
              "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
              " array([  3.  ,   5.76,   8.52,  11.28,  14.04,  16.8 ,  19.56,  22.32,\n",
              "         25.08,  27.84,  30.6 ,  33.36,  36.12,  38.88,  41.64,  44.4 ,\n",
              "         47.16,  49.92,  52.68,  55.44,  58.2 ,  60.96,  63.72,  66.48,\n",
              "         69.24,  72.  ,  74.76,  77.52,  80.28,  83.04,  85.8 ,  88.56,\n",
              "         91.32,  94.08,  96.84,  99.6 , 102.36, 105.12, 107.88, 110.64,\n",
              "        113.4 , 116.16, 118.92, 121.68, 124.44, 127.2 , 129.96, 132.72,\n",
              "        135.48, 138.24, 141.  ]),\n",
              " <a list of 50 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFUNJREFUeJzt3X+s3fV93/HnqxAoSUdtgmHMdmayWm0IWgKxwF2mKoMODEQxfwTJUTS8zJKliGzp1KkxizTUpJGINpUWKfGEgouJshBGk2EFE9dyQNWkQLgEwi+H+ZYwuIViJwZKixpK+t4f5+Pl5HLuvR87ts8xfj6ko/P9vr+f7/fzuV/d45e/P873pqqQJKnHL417AJKkY4ehIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSp24njHsDhdvrpp9eKFSvGPQxJOqY8+OCDP6qqJQu1e9OFxooVK5iamhr3MCTpmJLk//a08/SUJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSunWFRpJFSe5I8oMku5P8ZpLTkuxMsqe9L25tk+TGJNNJHkly/tB21rf2e5KsH6q/L8mjbZ0bk6TVR/YhSRqP3iONPwa+VVW/AbwH2A1sAnZV1UpgV5sHuAxY2V4bgc0wCADgOuBC4ALguqEQ2NzaHlhvTavP1YckaQwW/EZ4klOB3wL+LUBVvQa8lmQt8IHWbCtwL/ApYC1wa1UVcF87Sjmrtd1ZVfvbdncCa5LcC5xaVd9p9VuBK4G727ZG9aEjbMWmu0bWn77+iqM8EkmTpOdI453APuBPkjyU5EtJ3gacWVXPA7T3M1r7pcCzQ+vPtNp89ZkRdebpQ5I0Bj2hcSJwPrC5qs4D/pb5TxNlRK0Ood4tycYkU0mm9u3bdzCrSpIOQk9ozAAzVXV/m7+DQYi80E470d73DrVfPrT+MuC5BerLRtSZp4+fU1U3VdWqqlq1ZMmCD2mUJB2iBUOjqv4KeDbJr7fSxcATwDbgwB1Q64E72/Q24Op2F9Vq4OV2amkHcEmSxe0C+CXAjrbslSSr211TV8/a1qg+JElj0Pto9H8PfCXJScBTwMcYBM7tSTYAzwBXtbbbgcuBaeDV1paq2p/ks8ADrd1nDlwUBz4O3AKcwuAC+N2tfv0cfUiSxqArNKrqYWDViEUXj2hbwDVzbGcLsGVEfQo4d0T9x6P6kCSNx5vujzDp4Mx1a60kjeJjRCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHXrCo0kTyd5NMnDSaZa7bQkO5Psae+LWz1JbkwyneSRJOcPbWd9a78nyfqh+vva9qfbupmvD0nSeBzMkca/qqr3VtWqNr8J2FVVK4FdbR7gMmBle20ENsMgAIDrgAuBC4DrhkJgc2t7YL01C/QhSRqDX+T01Fpga5veClw5VL+1Bu4DFiU5C7gU2FlV+6vqRWAnsKYtO7WqvlNVBdw6a1uj+pAkjUFvaBTwZ0keTLKx1c6squcB2vsZrb4UeHZo3ZlWm68+M6I+Xx+SpDE4sbPd+6vquSRnADuT/GCethlRq0Ood2tBthHgHe94x8GsKkk6CF1HGlX1XHvfC3yDwTWJF9qpJdr73tZ8Blg+tPoy4LkF6stG1Jmnj9nju6mqVlXVqiVLlvT8SJKkQ7BgaCR5W5J/dGAauAR4DNgGHLgDaj1wZ5veBlzd7qJaDbzcTi3tAC5JsrhdAL8E2NGWvZJkdbtr6upZ2xrVhyRpDHpOT50JfKPdBXsi8D+q6ltJHgBuT7IBeAa4qrXfDlwOTAOvAh8DqKr9ST4LPNDafaaq9rfpjwO3AKcAd7cXwPVz9CFJGoMFQ6OqngLeM6L+Y+DiEfUCrpljW1uALSPqU8C5vX1IksbDb4RLkroZGpKkboaGJKlb7/c0dIxbsemucQ9B0puARxqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkbt2hkeSEJA8l+WabPzvJ/Un2JPlakpNa/eQ2P92WrxjaxrWt/mSSS4fqa1ptOsmmofrIPiRJ43EwRxqfBHYPzX8euKGqVgIvAhtafQPwYlX9GnBDa0eSc4B1wLuBNcAXWxCdAHwBuAw4B/hIaztfH5KkMegKjSTLgCuAL7X5ABcBd7QmW4Er2/TaNk9bfnFrvxa4rap+UlU/BKaBC9pruqqeqqrXgNuAtQv0IUkag94jjT8Cfg/4hzb/duClqnq9zc8AS9v0UuBZgLb85db+/9dnrTNXfb4+JEljsGBoJPkgsLeqHhwuj2haCyw7XPVRY9yYZCrJ1L59+0Y1kSQdBj1HGu8HPpTkaQanji5icOSxKMmJrc0y4Lk2PQMsB2jLfxXYP1yftc5c9R/N08fPqaqbqmpVVa1asmRJx48kSToUC4ZGVV1bVcuqagWDC9nfrqqPAvcAH27N1gN3tultbZ62/NtVVa2+rt1ddTawEvgu8ACwst0pdVLrY1tbZ64+JEljcOLCTeb0KeC2JH8APATc3Oo3A19OMs3gCGMdQFU9nuR24AngdeCaqvopQJJPADuAE4AtVfX4An1owqzYdNfI+tPXX3GURyLpSDqo0Kiqe4F72/RTDO58mt3m74Cr5lj/c8DnRtS3A9tH1Ef2IUkaD78RLknqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnq9os8Gl3HobkegS7p+OCRhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG4LhkaSX07y3STfT/J4kt9v9bOT3J9kT5KvJTmp1U9u89Nt+YqhbV3b6k8muXSovqbVppNsGqqP7EOSNB49Rxo/AS6qqvcA7wXWJFkNfB64oapWAi8CG1r7DcCLVfVrwA2tHUnOAdYB7wbWAF9MckKSE4AvAJcB5wAfaW2Zpw9J0hgsGBo18Ddt9i3tVcBFwB2tvhW4sk2vbfO05RcnSavfVlU/qaofAtPABe01XVVPVdVrwG3A2rbOXH1Iksag65pGOyJ4GNgL7AT+Anipql5vTWaApW16KfAsQFv+MvD24fqsdeaqv32ePmaPb2OSqSRT+/bt6/mRJEmHoCs0quqnVfVeYBmDI4N3jWrW3jPHssNVHzW+m6pqVVWtWrJkyagmkqTD4KDunqqql4B7gdXAoiQH/vLfMuC5Nj0DLAdoy38V2D9cn7XOXPUfzdOHJGkMeu6eWpJkUZs+BfhtYDdwD/Dh1mw9cGeb3tbmacu/XVXV6uva3VVnAyuB7wIPACvbnVInMbhYvq2tM1cfkqQx6Pkb4WcBW9tdTr8E3F5V30zyBHBbkj8AHgJubu1vBr6cZJrBEcY6gKp6PMntwBPA68A1VfVTgCSfAHYAJwBbqurxtq1PzdGHJGkMFgyNqnoEOG9E/SkG1zdm1/8OuGqObX0O+NyI+nZge28fkqTx8BvhkqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSerW88BCTaAVm+4aWX/6+iuO8kjmd6yMU1IfjzQkSd080niTmet/9pJ0OHikIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuC4ZGkuVJ7kmyO8njST7Z6qcl2ZlkT3tf3OpJcmOS6SSPJDl/aFvrW/s9SdYP1d+X5NG2zo1JMl8fkqTx6DnSeB343ap6F7AauCbJOcAmYFdVrQR2tXmAy4CV7bUR2AyDAACuAy4ELgCuGwqBza3tgfXWtPpcfUiSxmDB0Kiq56vqe236FWA3sBRYC2xtzbYCV7bptcCtNXAfsCjJWcClwM6q2l9VLwI7gTVt2alV9Z2qKuDWWdsa1YckaQwO6ppGkhXAecD9wJlV9TwMggU4ozVbCjw7tNpMq81XnxlRZ54+Zo9rY5KpJFP79u07mB9JknQQukMjya8Afwr8TlX99XxNR9TqEOrdquqmqlpVVauWLFlyMKtKkg5CV2gkeQuDwPhKVX29lV9op5Zo73tbfQZYPrT6MuC5BerLRtTn60OSNAY9d08FuBnYXVV/OLRoG3DgDqj1wJ1D9avbXVSrgZfbqaUdwCVJFrcL4JcAO9qyV5Ksbn1dPWtbo/qQJI1Bz597fT/wb4BHkzzcav8ZuB64PckG4BngqrZsO3A5MA28CnwMoKr2J/ks8EBr95mq2t+mPw7cApwC3N1ezNOHJGkMFgyNqvrfjL7uAHDxiPYFXDPHtrYAW0bUp4BzR9R/PKoPSdJ4+I1wSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUrefZU9Jht2LTXSPrT19/xVEeiaSD4ZGGJKmboSFJ6ubpqQk312kcSRoHjzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1G3B0EiyJcneJI8N1U5LsjPJnva+uNWT5MYk00keSXL+0DrrW/s9SdYP1d+X5NG2zo1JMl8fkqTx6TnSuAVYM6u2CdhVVSuBXW0e4DJgZXttBDbDIACA64ALgQuA64ZCYHNre2C9NQv0IUkakwVDo6r+HNg/q7wW2NqmtwJXDtVvrYH7gEVJzgIuBXZW1f6qehHYCaxpy06tqu9UVQG3ztrWqD4kSWNyqNc0zqyq5wHa+xmtvhR4dqjdTKvNV58ZUZ+vD0nSmBzuC+EZUatDqB9cp8nGJFNJpvbt23ewq0uSOh1qaLzQTi3R3ve2+gywfKjdMuC5BerLRtTn6+MNquqmqlpVVauWLFlyiD+SJGkhhxoa24ADd0CtB+4cql/d7qJaDbzcTi3tAC5JsrhdAL8E2NGWvZJkdbtr6upZ2xrVhyRpTBb8c69Jvgp8ADg9yQyDu6CuB25PsgF4BriqNd8OXA5MA68CHwOoqv1JPgs80Np9pqoOXFz/OIM7tE4B7m4v5ulDkjQmC4ZGVX1kjkUXj2hbwDVzbGcLsGVEfQo4d0T9x6P6kCSNj98IlyR1W/BIQ0feik13jXsIktTFIw1JUjdDQ5LUzdCQJHUzNCRJ3bwQroky300BT19/xVEciaRRDA0dM+YKFMNEOno8PSVJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnq5jfCjyL/bsaR4TfFpaPHIw1JUjdDQ5LUzdCQJHXzmsYR4LULSW9WhobetLxALh1+np6SJHUzNCRJ3SY+NJKsSfJkkukkm8Y9Hkk6nk30NY0kJwBfAP41MAM8kGRbVT0x3pHpWHawNyp4DUT6mYkODeACYLqqngJIchuwFjA0dNR4QV36mUkPjaXAs0PzM8CFYxqL9HM8YtHxaNJDIyNq9YZGyUZgY5v9myRPzrPN04EfHYaxHU3H2pgd7wj5/GHblPv3yDpex/tPexpNemjMAMuH5pcBz81uVFU3ATf1bDDJVFWtOjzDOzqOtTE73iPL8R5Zjnd+k3731APAyiRnJzkJWAdsG/OYJOm4NdFHGlX1epJPADuAE4AtVfX4mIclScetiQ4NgKraDmw/jJvsOo01YY61MTveI8vxHlmOdx6pesN1ZUmSRpr0axqSpAlyXIXGpD+SJMnyJPck2Z3k8SSfbPXTkuxMsqe9Lx73WIclOSHJQ0m+2ebPTnJ/G+/X2k0MEyHJoiR3JPlB28+/Ocn7N8l/bL8LjyX5apJfnrT9m2RLkr1JHhuqjdynGbixfQYfSXL+hIz3v7bfiUeSfCPJoqFl17bxPpnk0kkY79Cy/5Skkpze5o/4/j1uQmPokSSXAecAH0lyznhH9QavA79bVe8CVgPXtDFuAnZV1UpgV5ufJJ8Edg/Nfx64oY33RWDDWEY12h8D36qq3wDew2DcE7l/kywF/gOwqqrOZXAzyDomb//eAqyZVZtrn14GrGyvjcDmozTGYbfwxvHuBM6tqn8O/B/gWoD2+VsHvLut88X2b8nRdAtvHC9JljN4xNIzQ+Ujvn+Pm9Bg6JEkVfUacOCRJBOjqp6vqu+16VcY/IO2lME4t7ZmW4ErxzPCN0qyDLgC+FKbD3ARcEdrMjHjTXIq8FvAzQBV9VpVvcQE718GN6uckuRE4K3A80zY/q2qPwf2zyrPtU/XArfWwH3AoiRnHZ2RDowab1X9WVW93mbvY/CdMBiM97aq+klV/RCYZvBvyVEzx/4FuAH4PX7+C89HfP8eT6Ex6pEkS8c0lgUlWQGcB9wPnFlVz8MgWIAzxjeyN/gjBr+4/9Dm3w68NPQBnKT9/E5gH/An7XTal5K8jQndv1X1l8B/Y/A/yeeBl4EHmdz9O2yufXosfA7/HXB3m57I8Sb5EPCXVfX9WYuO+HiPp9DoeiTJJEjyK8CfAr9TVX897vHMJckHgb1V9eBweUTTSdnPJwLnA5ur6jzgb5mQU1GjtOsAa4GzgX8CvI3B6YfZJmX/9pjk3w+SfJrBaeKvHCiNaDbW8SZ5K/Bp4L+MWjyidljHezyFRtcjScYtyVsYBMZXqurrrfzCgUPM9r53XOOb5f3Ah5I8zeB030UMjjwWtdMpMFn7eQaYqar72/wdDEJkUvfvbwM/rKp9VfX3wNeBf8Hk7t9hc+3Tif0cJlkPfBD4aP3suwiTON5/xuA/Et9vn71lwPeS/GOOwniPp9CY+EeStOsBNwO7q+oPhxZtA9a36fXAnUd7bKNU1bVVtayqVjDYn9+uqo8C9wAfbs0mabx/BTyb5Ndb6WIGj9mfyP3L4LTU6iRvbb8bB8Y7kft3lrn26Tbg6naXz2rg5QOnscYpyRrgU8CHqurVoUXbgHVJTk5yNoMLzN8dxxgPqKpHq+qMqlrRPnszwPnt9/vI79+qOm5ewOUM7oz4C+DT4x7PiPH9SwaHko8AD7fX5QyuE+wC9rT308Y91hFj/wDwzTb9TgYfrGngfwInj3t8Q+N8LzDV9vH/AhZP8v4Ffh/4AfAY8GXg5Enbv8BXGVxz+XsG/4BtmGufMjh98oX2GXyUwZ1hkzDeaQbXAg587v77UPtPt/E+CVw2CeOdtfxp4PSjtX/9RrgkqdvxdHpKkvQLMjQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LU7f8BJYPcNf2OlhcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qZy7XXWCe1e2",
        "colab_type": "code",
        "outputId": "10f91594-37f2-41c3-e522-6354c50a8559",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(TRAIN_sum_len,bins=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.250000e+02, 3.401000e+03, 3.939740e+05, 4.570290e+05,\n",
              "        1.264969e+06, 5.713790e+05, 6.959900e+05, 1.701160e+05,\n",
              "        1.675970e+05, 3.639700e+04, 3.096300e+04, 5.518000e+03,\n",
              "        2.962000e+03, 2.402000e+03, 4.720000e+02, 3.750000e+02,\n",
              "        7.100000e+01, 5.800000e+01, 1.500000e+01, 1.100000e+01,\n",
              "        4.000000e+00, 1.300000e+01, 1.000000e+00, 3.000000e+00,\n",
              "        1.000000e+00, 2.000000e+00, 1.000000e+00, 1.000000e+00,\n",
              "        0.000000e+00, 2.000000e+00, 0.000000e+00, 1.000000e+00,\n",
              "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
              "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
              "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
              "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
              "        0.000000e+00, 4.000000e+00]),\n",
              " array([ 3.  ,  4.46,  5.92,  7.38,  8.84, 10.3 , 11.76, 13.22, 14.68,\n",
              "        16.14, 17.6 , 19.06, 20.52, 21.98, 23.44, 24.9 , 26.36, 27.82,\n",
              "        29.28, 30.74, 32.2 , 33.66, 35.12, 36.58, 38.04, 39.5 , 40.96,\n",
              "        42.42, 43.88, 45.34, 46.8 , 48.26, 49.72, 51.18, 52.64, 54.1 ,\n",
              "        55.56, 57.02, 58.48, 59.94, 61.4 , 62.86, 64.32, 65.78, 67.24,\n",
              "        68.7 , 70.16, 71.62, 73.08, 74.54, 76.  ]),\n",
              " <a list of 50 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFJNJREFUeJzt3X+MXeWd3/H3p/ZCSLbE/HAiaqOaKFY2JNokZEScplqlsAUDUcwfRAKtFiu1ZDUibdKstDFdqahJVwK1WnaREiQUWKCKIJTNFish8VpAtOoq/BiHLL8c1lOgMIXFQwwkXdSwZL/94z7TXMydGTyP4d7B75d0dc/5nuec5ztzjT/cc869TlUhSVKPfzTuBiRJK59hIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSp2+pxN/BmOfHEE2vDhg3jbkOSVpQ9e/Y8V1Vrlxp3xITJhg0bmJ6eHncbkrSiJPlfr2ecp7kkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3Y6YT8BPug07vjuy/sTl573JnUjSofOdiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqtmSYJLkuyf4kDw3V/nOSnyR5IMmfJ1kztO3SJDNJHk1y9lB9c6vNJNkxVD8lyT1J9iX5VpKjWv3otj7Ttm9Yag5J0ni8nncm1wObD6rtBj5YVb8J/A1wKUCSU4ELgQ+0fb6eZFWSVcDXgHOAU4GL2liAK4Arq2oj8DywrdW3Ac9X1XuBK9u4Bec4xJ9bknQYLRkmVfWXwIGDan9RVa+01buB9W15C3BzVf2iqh4HZoDT22Omqh6rqpeBm4EtSQKcAdza9r8BOH/oWDe05VuBM9v4heaQJI3J4bhm8q+A77XldcBTQ9tmW22h+gnAC0PBNF9/1bHa9hfb+IWO9RpJtieZTjI9Nze3rB9OkrS0rjBJ8gfAK8A350sjhtUy6ss51muLVddU1VRVTa1du3bUEEnSYbDs7+ZKshX4FHBmVc3/ZT4LnDw0bD3wdFseVX8OWJNkdXv3MTx+/lizSVYD72Rwum2xOSRJY7CsdyZJNgNfBj5dVS8NbdoJXNjuxDoF2AjcC9wHbGx3bh3F4AL6zhZCdwEXtP23ArcNHWtrW74AuLONX2gOSdKYLPnOJMlNwCeBE5PMApcxuHvraGD34Jo4d1fVv66qh5PcAjzC4PTXJVX1y3aczwO7gFXAdVX1cJviy8DNSf4TcD9wbatfC/zXJDMM3pFcCLDYHJKk8civzlC9tU1NTdX09PS421iQX0EvaRIl2VNVU0uN8xPwkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSeq2ZJgkuS7J/iQPDdWOT7I7yb72fFyrJ8lVSWaSPJDktKF9trbx+5JsHap/NMmDbZ+rkmS5c0iSxuP1vDO5Hth8UG0HcEdVbQTuaOsA5wAb22M7cDUMggG4DPgYcDpw2Xw4tDHbh/bbvJw5JEnjs2SYVNVfAgcOKm8BbmjLNwDnD9VvrIG7gTVJTgLOBnZX1YGqeh7YDWxu246tqh9WVQE3HnSsQ5lDkjQmy71m8u6qegagPb+r1dcBTw2Nm221xeqzI+rLmUOSNCaH+wJ8RtRqGfXlzPHagcn2JNNJpufm5pY4rCRpuZYbJs/On1pqz/tbfRY4eWjceuDpJerrR9SXM8drVNU1VTVVVVNr1649pB9QkvT6LTdMdgLzd2RtBW4bql/c7rjaBLzYTlHtAs5Kcly78H4WsKtt+3mSTe0urosPOtahzCFJGpPVSw1IchPwSeDEJLMM7sq6HLglyTbgSeAzbfjtwLnADPAS8FmAqjqQ5KvAfW3cV6pq/qL+5xjcMXYM8L324FDnkCSNz5JhUlUXLbDpzBFjC7hkgeNcB1w3oj4NfHBE/aeHOockaTz8BLwkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuXWGS5N8leTjJQ0luSvK2JKckuSfJviTfSnJUG3t0W59p2zcMHefSVn80ydlD9c2tNpNkx1B95BySpPFYdpgkWQf8W2Cqqj4IrAIuBK4ArqyqjcDzwLa2yzbg+ap6L3BlG0eSU9t+HwA2A19PsirJKuBrwDnAqcBFbSyLzCFJGoPe01yrgWOSrAbeDjwDnAHc2rbfAJzflre0ddr2M5Ok1W+uql9U1ePADHB6e8xU1WNV9TJwM7Cl7bPQHJKkMVh2mFTV/wb+C/AkgxB5EdgDvFBVr7Rhs8C6trwOeKrt+0obf8Jw/aB9FqqfsMgckqQx6DnNdRyDdxWnAP8EeAeDU1IHq/ldFth2uOqjetyeZDrJ9Nzc3KghkqTDoOc0128Dj1fVXFX9PfBt4J8Ba9ppL4D1wNNteRY4GaBtfydwYLh+0D4L1Z9bZI5XqaprqmqqqqbWrl3b8aNKkhbTEyZPApuSvL1dxzgTeAS4C7igjdkK3NaWd7Z12vY7q6pa/cJ2t9cpwEbgXuA+YGO7c+soBhfpd7Z9FppDkjQGPddM7mFwEfxHwIPtWNcAXwa+lGSGwfWNa9su1wIntPqXgB3tOA8DtzAIou8Dl1TVL9s1kc8Du4C9wC1tLIvMIUkagwz+R/+tb2pqqqanp8fdxoI27PjuyPoTl5/3JnciSb+SZE9VTS01zk/AS5K6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6rZ66SGaRH4xpKRJ4jsTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHXrCpMka5LcmuQnSfYm+XiS45PsTrKvPR/XxibJVUlmkjyQ5LSh42xt4/cl2TpU/2iSB9s+VyVJq4+cQ5I0Hr3vTP4E+H5V/QbwIWAvsAO4o6o2Ane0dYBzgI3tsR24GgbBAFwGfAw4HbhsKByubmPn99vc6gvNIUkag2WHSZJjgd8CrgWoqper6gVgC3BDG3YDcH5b3gLcWAN3A2uSnAScDeyuqgNV9TywG9jcth1bVT+sqgJuPOhYo+aQJI1BzzuT9wBzwJ8muT/JN5K8A3h3VT0D0J7f1cavA54a2n+21Rarz46os8gckqQx6AmT1cBpwNVV9RHg71j8dFNG1GoZ9dctyfYk00mm5+bmDmVXSdIh6AmTWWC2qu5p67cyCJdn2ykq2vP+ofEnD+2/Hnh6ifr6EXUWmeNVquqaqpqqqqm1a9cu64eUJC1t2WFSVX8LPJXkfa10JvAIsBOYvyNrK3BbW94JXNzu6toEvNhOUe0CzkpyXLvwfhawq237eZJN7S6uiw861qg5JElj0PvP9v4b4JtJjgIeAz7LIKBuSbINeBL4TBt7O3AuMAO81MZSVQeSfBW4r437SlUdaMufA64HjgG+1x4Aly8whyRpDDK4Ueqtb2pqqqanp8fdxoIW+jfdD5X/BrykwynJnqqaWmqcn4CXJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd16/w14HaLD9c/zStIk8Z2JJKmbYSJJ6maYSJK6GSaSpG6GiSSpW3eYJFmV5P4k32nrpyS5J8m+JN9KclSrH93WZ9r2DUPHuLTVH01y9lB9c6vNJNkxVB85hyRpPA7HO5MvAHuH1q8ArqyqjcDzwLZW3wY8X1XvBa5s40hyKnAh8AFgM/D1FlCrgK8B5wCnAhe1sYvNIUkag64wSbIeOA/4RlsPcAZwaxtyA3B+W97S1mnbz2zjtwA3V9UvqupxYAY4vT1mquqxqnoZuBnYssQckqQx6H1n8sfA7wP/0NZPAF6oqlfa+iywri2vA54CaNtfbOP/f/2gfRaqLzbHqyTZnmQ6yfTc3Nxyf0ZJ0hKW/Qn4JJ8C9lfVniSfnC+PGFpLbFuoPiroFhv/2mLVNcA1AFNTUyPHvFH8pLukI0nP16l8Avh0knOBtwHHMninsibJ6vbOYT3wdBs/C5wMzCZZDbwTODBUnze8z6j6c4vMIUkag2Wf5qqqS6tqfVVtYHAB/c6q+h3gLuCCNmwrcFtb3tnWadvvrKpq9Qvb3V6nABuBe4H7gI3tzq2j2hw72z4LzSFJGoM34nMmXwa+lGSGwfWNa1v9WuCEVv8SsAOgqh4GbgEeAb4PXFJVv2zvOj4P7GJwt9gtbexic0iSxuCwfGtwVf0A+EFbfozBnVgHj/m/wGcW2P8PgT8cUb8duH1EfeQckqTx8BPwkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSeq27DBJcnKSu5LsTfJwki+0+vFJdifZ156Pa/UkuSrJTJIHkpw2dKytbfy+JFuH6h9N8mDb56okWWwOSdJ49LwzeQX4vap6P7AJuCTJqcAO4I6q2gjc0dYBzgE2tsd24GoYBANwGfAx4HTgsqFwuLqNnd9vc6svNIckaQyWHSZV9UxV/agt/xzYC6wDtgA3tGE3AOe35S3AjTVwN7AmyUnA2cDuqjpQVc8Du4HNbduxVfXDqirgxoOONWoOSdIYHJZrJkk2AB8B7gHeXVXPwCBwgHe1YeuAp4Z2m221xeqzI+osMockaQy6wyTJrwN/Bnyxqn622NARtVpG/VB6255kOsn03NzcoewqSToEXWGS5NcYBMk3q+rbrfxsO0VFe97f6rPAyUO7rweeXqK+fkR9sTlepaquqaqpqppau3bt8n5ISdKSeu7mCnAtsLeq/mho005g/o6srcBtQ/WL211dm4AX2ymqXcBZSY5rF97PAna1bT9PsqnNdfFBxxo1hyRpDFZ37PsJ4HeBB5P8uNX+PXA5cEuSbcCTwGfattuBc4EZ4CXgswBVdSDJV4H72rivVNWBtvw54HrgGOB77cEic0iSxmDZYVJV/4PR1zUAzhwxvoBLFjjWdcB1I+rTwAdH1H86ag5J0nj0vDPRBNqw47uHNP6Jy897gzqRdCTx61QkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3VaPuwGN14Yd3x1Zf+Ly897kTiStZIaJRjJkJB2KFX2aK8nmJI8mmUmyY9z9SNKRasWGSZJVwNeAc4BTgYuSnDreriTpyLRiwwQ4HZipqseq6mXgZmDLmHuSpCPSSr5msg54amh9FvjYm93EQtcW3qoO58/r9RfprWMlh0lG1OpVA5LtwPa2+n+SPNqWTwSeewN7O1xWQp/L7jFXHOZOFrcSfpewMvpcCT2CfR4u//T1DFrJYTILnDy0vh54enhAVV0DXHPwjkmmq2rqjW2v30rocyX0CPZ5OK2EHsE+32wr+ZrJfcDGJKckOQq4ENg55p4k6Yi0Yt+ZVNUrST4P7AJWAddV1cNjbkuSjkgrNkwAqup24PZl7PqaU18TaiX0uRJ6BPs8nFZCj2Cfb6pU1dKjJElaxEq+ZiJJmhBHVJhM6tevJLkuyf4kDw3Vjk+yO8m+9nzcOHtsPZ2c5K4ke5M8nOQLk9ZrkrcluTfJX7ce/2Orn5Lkntbjt9pNG2OXZFWS+5N8p61PXJ9JnkjyYJIfJ5lutYl5zYf6XJPk1iQ/aX9GPz5JfSZ5X/sdzj9+luSLk9RjjyMmTCb861euBzYfVNsB3FFVG4E72vq4vQL8XlW9H9gEXNJ+h5PU6y+AM6rqQ8CHgc1JNgFXAFe2Hp8Hto2xx2FfAPYOrU9qn/+iqj48dAvrJL3m8/4E+H5V/QbwIQa/14nps6oebb/DDwMfBV4C/nySeuxSVUfEA/g4sGto/VLg0nH3NdTPBuChofVHgZPa8knAo+PucUTPtwH/clJ7Bd4O/IjBNyM8B6we9WdhjP2tZ/CXxxnAdxh8EHcS+3wCOPGg2kS95sCxwOO068CT2udQX2cBfzXJPR7q44h5Z8Lor19ZN6ZeXo93V9UzAO35XWPu51WSbAA+AtzDhPXaTh39GNgP7Ab+J/BCVb3ShkzKa//HwO8D/9DWT2Ay+yzgL5Lsad8qARP2mgPvAeaAP22nDb+R5B1MXp/zLgRuasuT2uMhOZLCZMmvX9Hrk+TXgT8DvlhVPxt3Pwerql/W4FTCegZfCPr+UcPe3K5eLcmngP1VtWe4PGLoJPwZ/URVncbgFPElSX5r3A2NsBo4Dbi6qj4C/B0TerqoXQf7NPDfxt3L4XQkhcmSX78yYZ5NchJAe94/5n4ASPJrDILkm1X17VaeyF6r6gXgBwyu76xJMv+5qkl47T8BfDrJEwy+8foMBu9UJq1Pqurp9ryfwTn+05m813wWmK2qe9r6rQzCZdL6hEEo/6iqnm3rk9jjITuSwmSlff3KTmBrW97K4PrEWCUJcC2wt6r+aGjTxPSaZG2SNW35GOC3GVyIvQu4oA0b+++zqi6tqvVVtYHBn8U7q+p3mLA+k7wjyT+eX2Zwrv8hJug1B6iqvwWeSvK+VjoTeIQJ67O5iF+d4oLJ7PHQjfuizZv5AM4F/obBOfQ/GHc/Q33dBDwD/D2D/8PaxuD8+R3AvvZ8/AT0+c8ZnHZ5APhxe5w7Sb0Cvwnc33p8CPgPrf4e4F5ghsHphaPH/fsc6vmTwHcmsc/Wz1+3x8Pz/91M0ms+1OuHgen22v934LhJ65PBTSE/Bd45VJuoHpf78BPwkqRuR9JpLknSG8QwkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUrf/BwX86hSbR9aHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7YqRGGyne1e9",
        "colab_type": "code",
        "outputId": "5d7f36a4-66a2-40a5-f1b4-0298dc25c84a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = np.where(np.logical_and(TRAIN_in_len>=20,np.logical_and(TRAIN_in_len<=50,np.logical_and(TRAIN_sum_len>=6,TRAIN_sum_len<=14))))\n",
        "print(len(a[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3313699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kDrDeX1Re1fM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_indices = np.random.randint(len(a[0]),size=1000000)\n",
        "indices = a[0][data_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRZ1eTAxe1fT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_text = lines_train_in[indices]\n",
        "data_sum = lines_train_sum[indices]\n",
        "output_len = TRAIN_sum_len[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SP-4VR-Ue1fY",
        "colab_type": "code",
        "outputId": "35ae7e61-5298-4a3f-baae-0f11012136d5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dict['pad']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.93299 , -0.14931 , -0.35147 , -0.3502  , -0.1147  ,  0.52476 ,\n",
              "       -0.03901 , -0.27993 , -0.40184 ,  0.59111 ,  0.62797 , -0.10296 ,\n",
              "        0.043983, -0.10143 ,  0.25852 ,  0.089608, -0.098707, -0.012967,\n",
              "        0.94916 , -0.67287 ,  0.18841 , -0.045516,  0.13782 , -0.10224 ,\n",
              "        0.83805 ,  0.46198 , -0.14284 ,  0.52084 ,  0.15953 , -0.34714 ,\n",
              "       -0.29097 ,  0.6335  ,  0.68245 ,  0.95167 ,  0.12976 ,  0.53185 ,\n",
              "       -0.25138 ,  0.04886 , -0.021347, -0.65376 , -0.3953  , -0.076416,\n",
              "        0.3537  , -0.57907 , -0.2226  , -0.37824 , -0.48364 ,  0.78115 ,\n",
              "       -0.12534 ,  0.32698 , -0.43481 ,  0.66491 , -0.17452 ,  1.1382  ,\n",
              "       -0.08637 , -0.7826  , -0.076904, -0.075961,  1.5143  ,  0.39427 ,\n",
              "       -0.25568 ,  0.42727 , -0.15364 ,  1.0555  ,  0.15692 ,  0.4321  ,\n",
              "        0.30187 , -0.89785 , -0.32137 , -0.34213 , -0.17997 ,  0.20478 ,\n",
              "       -0.66434 , -0.21832 , -0.49102 , -0.027664,  0.24862 ,  0.99477 ,\n",
              "       -0.1438  ,  0.87516 , -0.090748, -0.41021 , -0.312   , -0.81517 ,\n",
              "       -0.59217 ,  0.19257 ,  1.0619  ,  0.90818 ,  0.11364 ,  0.075981,\n",
              "        0.37005 ,  0.76865 ,  0.50803 , -0.05541 ,  0.037133,  0.2934  ,\n",
              "        0.28853 ,  0.40952 , -0.016286,  0.22317 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "nqpltrple1fh",
        "colab_type": "code",
        "outputId": "daf59468-4fe7-469c-b555-8c8a7cd908a8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for line in data_text:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)\n",
        "for line in data_sum:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "embed_dim = 100\n",
        "words_found = 0\n",
        "words_OOV = 0\n",
        "word2idx = {}\n",
        "word2idx['<pad>'] = 0\n",
        "for index, word in enumerate(vocab):\n",
        "    word2idx[word] = index+1\n",
        "weight_embedding = np.zeros([len(vocab)+1,embed_dim],dtype='float32')\n",
        "for i, word in enumerate(vocab):\n",
        "    try:\n",
        "        weight_embedding[i,:] = embedding_dict[word]\n",
        "        words_found += 1\n",
        "    except:\n",
        "        weight_embedding[i,:] = np.random.normal(scale=0.6, size=[embed_dim,])\n",
        "        words_OOV += 1\n",
        "\n",
        "print(\"{} words found\".format(words_found))\n",
        "print(\"{} OOV words\".format(words_OOV))\n",
        "idx2word = dict((v,k) for k,v in word2idx.items())\n",
        "print(\"{} words in the vocabulary\".format(len(vocab)))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93739 words found\n",
            "1845 OOV words\n",
            "95584 words in the vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qpuZXnpDe1fq",
        "colab_type": "code",
        "outputId": "03c8b306-5d22-4e74-9255-5fe60d66c4bf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_ints = []\n",
        "summary_ints = []\n",
        "for each in data_text:\n",
        "    sentence = []\n",
        "    for word in each.split():\n",
        "        sentence.extend([word2idx[word]])\n",
        "    text_ints.append(sentence)\n",
        "for each in data_sum:\n",
        "    sentence = []\n",
        "    for word in each.split():\n",
        "        sentence.extend([word2idx[word]])\n",
        "    summary_ints.append(sentence)\n",
        "\n",
        "text_len = 50\n",
        "summary_len = 14\n",
        "text_feature = np.zeros((len(text_ints),text_len),dtype=int)\n",
        "summary_feature = np.zeros((len(summary_ints),summary_len), dtype=int)\n",
        "for i, row in enumerate(text_ints):\n",
        "    text_feature[i,:len(row)] = np.array(row)[:text_len]\n",
        "for i, row in enumerate(summary_ints):\n",
        "    summary_feature[i,:len(row)] = np.array(row)[:summary_len]\n",
        "\n",
        "split_frac = 0.8 \n",
        "split_index = int(len(text_feature)*0.8)\n",
        "train_text, test_text = text_feature[:split_index],text_feature[split_index:]\n",
        "train_summary, test_summary = summary_feature[:split_index],summary_feature[split_index:]\n",
        "\n",
        "\n",
        "print(\"\\t\\t\\tData Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_text.shape), \n",
        "      \"\\nTest set: \\t\\t{}\".format(test_text.shape))\n",
        "\n",
        "print(\"\\n\\n\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_summary.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_summary.shape))\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tData Shapes:\n",
            "Train set: \t\t(800000, 50) \n",
            "Test set: \t\t(200000, 50)\n",
            "\n",
            "\n",
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(800000, 14) \n",
            "Test set: \t\t(200000, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mGhY9bhEe1gJ",
        "colab_type": "code",
        "outputId": "7836d56a-deae-40f9-8b2d-f2b31f033f53",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.savez('data1M.npz',train_text=train_text,test_text=test_text,\n",
        "         train_summary=train_summary,test_summary=test_summary,\n",
        "         word2idx = word2idx,idx2word=idx2word, embed_matrix=weight_embedding, output_length = output_len)\n",
        "print(\"Data Saved!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HI0EYj8vfUq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0166f375-6ba3-4d12-b01f-6750490898b6"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "import os\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q torch==1.0.0 torchvision\n",
        "import torch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x60dd6000 @  0x7f63c5ed12a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dkkGZ0rUoAYh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ovnv3XZpe1gQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a3d18e82-1c73-48b7-e01f-110308f840a3"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string \n",
        "import os\n",
        "import re\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(os.path)\n",
        "\n",
        "\n",
        "data = np.load('/content/drive/My Drive/Colab Notebooks/data1M.npz')\n",
        "train_text = data['train_text']\n",
        "test_text = data['test_text']\n",
        "train_summary = data['train_summary']\n",
        "test_summary = data['test_summary']\n",
        "word2idx = data['word2idx']\n",
        "word2idx = dict(word2idx.item())\n",
        "idx2word = data['idx2word']\n",
        "idx2word = dict(idx2word.item())\n",
        "weight_embedding = data['embed_matrix']\n",
        "weight_embedding = torch.from_numpy(weight_embedding)\n",
        "output_length = data['output_length']\n",
        "split_index = int(len(test_text)*0.5)\n",
        "val_text, test_text  = test_text[:split_index],test_text[split_index:]\n",
        "val_summary, test_summary  = test_summary[:split_index],test_summary[split_index:]\n",
        "print('Data Loaded!')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "<module 'posixpath' from '/usr/lib/python3.6/posixpath.py'>\n",
            "Data Loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sM9G8YV8e1ge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def create_emb(weight_matrix, non_trainable=False):\n",
        "    emb_layer = torch.nn.Embedding(weight_matrix.shape[0],weight_matrix.shape[1])\n",
        "    emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "    return emb_layer\n",
        "    \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,kernel_size,filter_size,dropout,num_hidden,layers,weight_matrix,batch_size,device):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_size = filter_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        \n",
        "        \n",
        "        self.conv1 = torch.nn.Conv1d(100,self.filter_size,self.kernel_size[0],stride=1,padding=0)\n",
        "        self.conv2 = torch.nn.Conv1d(100,self.filter_size,self.kernel_size[1],stride=1,padding=1)\n",
        "        self.conv3 = torch.nn.Conv1d(100,self.filter_size,self.kernel_size[2],stride=1,padding=2)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        self.gru = torch.nn.GRU(input_size = self.filter_size,hidden_size = self.num_hidden,num_layers = self.layers,\n",
        "                               dropout=0.5,bidirectional=True)\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.filter_size,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=True)\n",
        "    def forward(self,x,hidden):\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x1 = torch.tanh(self.dropout(self.conv1(x)))\n",
        "        x2 = torch.tanh(self.dropout(self.conv2(x)))\n",
        "        x3 = torch.tanh(self.dropout(self.conv3(x)))\n",
        "        output,(h_hidden,c_hidden) = self.lstm((x1+x2+x3).permute(0,2,1),hidden)\n",
        "        h = torch.cat((h_hidden[1,:,:],h_hidden[2,:,:]),1)\n",
        "        c = torch.cat((c_hidden[1,:,:],c_hidden[2,:,:]),1)\n",
        "        hidden = (h.unsqueeze(0),c.unsqueeze(0))\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers*2, self.batch_size, self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers*2, self.batch_size, self.num_hidden).zero_())\n",
        "        return hidden\n",
        "        \n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iOzAegfHe1gs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self,num_hidden,dropout,vocab_size,batch_size,layers,weight_matrix,embed_dims,device):\n",
        "        super(AttentionDecoder,self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.dropout = dropout\n",
        "        self.layers = layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embed_dims = embed_dims\n",
        "        self.device = device\n",
        "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
        "        \n",
        "        self.W_enc = Variable(torch.randn(self.num_hidden,self.num_hidden,device=self.device), requires_grad=True)\n",
        "        self.W_dec = Variable(torch.randn(self.num_hidden,self.num_hidden,device=self.device), requires_grad=True)\n",
        "        \n",
        "        self.W_proj =  Variable(torch.randn(self.embed_dims,2*self.num_hidden,device=self.device), requires_grad=True)\n",
        "        \n",
        "        self.pointer = torch.nn.Linear(2*self.num_hidden,1)\n",
        "        self.output_layer = torch.nn.Linear(2*self.num_hidden,self.vocab_size)\n",
        "        self.output_layer.weight = torch.nn.Parameter(weight_matrix.to(self.device).mm(self.W_proj))\n",
        "        \n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.num_hidden+self.embed_dims,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=False)\n",
        "        \n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "        self.device = device\n",
        "        \n",
        "        \n",
        "    def forward(self,x,enc_out,hidden,dec_hidden,text):\n",
        "        # decoder\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        x = self.dropout_layer(x)\n",
        "        \n",
        "        dec_a = hidden[0].permute(1,0,2)\n",
        "        #print(\"Input Shape: \",x.shape)\n",
        "        enc_score = torch.einsum('bmn,nn,ban->bma',[enc_out,self.W_enc,dec_a])\n",
        "        #print('Encoder Score Shape: ',enc_score.shape)\n",
        "        enc_weight = self.softmax(enc_score)\n",
        "        #print('Encoder Weight Shape: ',enc_weight.shape)\n",
        "        #print('Encoder Encoder Output Shape: ',enc_out.shape)\n",
        "        enc_context = torch.mul(enc_weight,enc_out)\n",
        "        #print('Encoder Context Shape: ',enc_context.shape)\n",
        "        enc_context = enc_context.sum(1)\n",
        "        enc_context.unsqueeze_(1)\n",
        "        #print('Encoder Context Shape: ',enc_context.shape)\n",
        "        x = torch.cat((x,enc_context),2)\n",
        "        \n",
        "        d_output, hidden = self.lstm(x,hidden)\n",
        "        \n",
        "        #dec_output = d_output[:,-1,:].unsqueeze(1)\n",
        "        # encoder attention\n",
        "        #enc_score = torch.einsum('bmn,nn,ban->bma',[enc_out,self.W_enc,dec_output])\n",
        "        #print('Encoder Score Shape: ',enc_score.shape)\n",
        "        #enc_weight = self.softmax(enc_score)\n",
        "        #print('Encoder Weight Shape: ',enc_weight.shape)\n",
        "        #print('Encoder Encoder Output Shape: ',enc_out.shape)\n",
        "        #enc_context = torch.mul(enc_weight,enc_out)\n",
        "        #print('Encoder Context Shape: ',enc_context.shape)\n",
        "        #enc_context = enc_context.sum(1)\n",
        "        #print('Encoder Context Shape: ',enc_context.shape)\n",
        "        #enc_context.squeeze_(1)\n",
        "        #print('Encoder Context Shape: ',enc_context.shape)\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        # decoder attention\n",
        "        #dec_score = torch.einsum('bmn,nn,ban->bma',[dec_hidden,self.W_dec,d_output])\n",
        "        #print('Decoder Score Shape: ',dec_score.shape)\n",
        "        #dec_weight = self.softmax(dec_score)\n",
        "        #print('Decoder Weight Shape: ',dec_weight.shape)\n",
        "        #print('Decoder Encoder Output Shape: ',dec_output.shape)\n",
        "        #dec_context = torch.mul(dec_weight,dec_hidden)\n",
        "        #print('Decoder Context Shape: ',dec_context.shape)\n",
        "        #dec_context = dec_context.sum(1)\n",
        "        #print('Decoder Context Shape: ',dec_context.shape)\n",
        "        #dec_context.squeeze_(1)\n",
        "        #print('Decoder Context Shape: ',dec_context.shape)\n",
        "        \n",
        "        #dec_hidden = torch.cat((dec_hidden,dec_output),1)\n",
        "        \n",
        "        output = torch.cat((d_output.squeeze(1),enc_context.squeeze(1)),1)\n",
        "        #print(\"Output Shape: \",output.shape)\n",
        "        # pointer-generator\n",
        "        p_pointer = self.pointer(output).squeeze(1)\n",
        "        #print(\"Pointer Shape: \",p_pointer.shape)\n",
        "        #print(\"Pointer: \",p_pointer)\n",
        "        p_pointer = self.sig(p_pointer)\n",
        "        p_gen = (1-p_pointer)\n",
        "        pointer_prob = torch.zeros([self.batch_size,self.vocab_size],device=self.device)\n",
        "        for i in range(self.batch_size):\n",
        "            pointer_prob[i,text[i,:]] = enc_weight[i,:,0]\n",
        "        generator_prob = self.output_layer(output)\n",
        "        #print('Pointer Probability: ',p_pointer.shape)\n",
        "        #print('Pointer: ',pointer_prob.shape)\n",
        "        #print('Generator Probability: ',p_gen.shape)\n",
        "        #print('Generator: ',generator_prob.shape)\n",
        "        output_probability = F.log_softmax(torch.mul(p_pointer.unsqueeze(1),pointer_prob) + torch.mul(p_gen.unsqueeze(1),generator_prob),1)\n",
        "        \n",
        "        return output_probability, hidden, dec_hidden\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers, self.batch_size, 2*self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers, self.batch_size, 2*self.num_hidden).zero_())\n",
        "        return hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oAgfyC4ae1gz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kernel_size = [1,3,5]\n",
        "filter_size = 50\n",
        "dropout = 0.5\n",
        "num_hidden = 200\n",
        "enc_layers = 2\n",
        "batch_size = 64\n",
        "vocab_size = len(word2idx)\n",
        "dec_layers = 1\n",
        "embed_dims = 100\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,kernel_size,filter_size,dropout,num_hidden,enc_layers,weight_embedding,batch_size,\n",
        "                vocab_size,dec_layers,embed_dims,device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.encoder = Encoder(kernel_size,filter_size,dropout,num_hidden,enc_layers,weight_embedding,batch_size,device)\n",
        "        self.decoder = AttentionDecoder(2*num_hidden,dropout,vocab_size,batch_size,dec_layers,weight_embedding,embed_dims,device)\n",
        "    \n",
        "    def forward(self,x,target,e_hidden,criterion,train_mode=True):\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1)\n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden)\n",
        "        dec_hidden = enc_hidden[0].permute(1,0,2)\n",
        "        d_hidden = enc_hidden\n",
        "        dec_input = target[:,0]\n",
        "        for t in range(1,target.shape[1]):\n",
        "            logits, d_hidden,dec_hidden = self.decoder(dec_input,enc_output,d_hidden,dec_hidden,x)\n",
        "            ## fix errors in hidden state\n",
        "            if train_mode == True:\n",
        "                dec_input = target[:,t]\n",
        "            else:\n",
        "                dec_input = torch.argmax(logits,dim=1)\n",
        "                \n",
        "            \n",
        "            loss += criterion(logits,target[:,t])\n",
        "            #print(prediction.shape)\n",
        "            #print(torch.argmax(logits,dim=1).shape)\n",
        "            prediction = torch.cat((prediction,torch.argmax(logits,dim=0).unsqueeze(1)),0)\n",
        "        return loss, prediction\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YD6zWGZ_e1g4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(x, y,batch_size=100):\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SP_-cvZae1g9",
        "colab_type": "code",
        "outputId": "5fb62787-dbd0-42a4-b489-9feba8751579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1091
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.0007\n",
        "model = Seq2Seq(kernel_size,filter_size,dropout,num_hidden,enc_layers,weight_embedding,batch_size,\n",
        "                vocab_size,dec_layers,embed_dims,device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "counter = 0\n",
        "loss = []\n",
        "for e in range(epochs):\n",
        "    p = np.random.permutation(train_summary.shape[0])\n",
        "    train_text = train_text[p,:]\n",
        "    train_summary = train_summary[p,:]\n",
        "    e_hidden = model.encoder.init_hidden()\n",
        "    \n",
        "    for x,y in get_batches(train_text,train_summary,batch_size):\n",
        "        model.train()\n",
        "        x = torch.from_numpy(x).to(device)\n",
        "        y = torch.from_numpy(y).to(device)\n",
        "        counter += 1\n",
        "        e_hidden = tuple([each.data for each in e_hidden])\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        l,prediction = model(x,y,e_hidden,criterion,train_mode=True)\n",
        "        loss.append(l.item())\n",
        "        l.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),5)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%50 == 0:\n",
        "          print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
        "                      \"\\tStep: {} \".format(counter),\n",
        "                      \"\\tLoss: {:.4f} \".format(l.item()))\n",
        "        if counter%3000 == 0:\n",
        "            with torch.no_grad():\n",
        "              loss_val = []\n",
        "              model.eval()\n",
        "              val_hidden = model.encoder.init_hidden()\n",
        "              for x_val, y_val in get_batches(test_text,test_summary,batch_size):\n",
        "                x_val = torch.from_numpy(x_val).to(device)\n",
        "                y_val = torch.from_numpy(y_val).to(device)\n",
        "                val_hidden = tuple([each.data for each in val_hidden])\n",
        "                val_loss,prediction = model(x,y,e_hidden,criterion,train_mode=False)\n",
        "                loss_val.append(val_loss.item())\n",
        "              model.train()\n",
        "              print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
        "                    \"\\t\\t\\t\\tVal Loss: {:.4f}\".format(np.mean(loss_val)))\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20  \tStep: 50  \tLoss: 91.0881 \n",
            "Epoch: 1/20  \tStep: 100  \tLoss: 90.5048 \n",
            "Epoch: 1/20  \tStep: 150  \tLoss: 84.4638 \n",
            "Epoch: 1/20  \tStep: 200  \tLoss: 86.6615 \n",
            "Epoch: 1/20  \tStep: 250  \tLoss: 82.4318 \n",
            "Epoch: 1/20  \tStep: 300  \tLoss: 83.4548 \n",
            "Epoch: 1/20  \tStep: 350  \tLoss: 81.2210 \n",
            "Epoch: 1/20  \tStep: 400  \tLoss: 80.3532 \n",
            "Epoch: 1/20  \tStep: 450  \tLoss: 83.1433 \n",
            "Epoch: 1/20  \tStep: 500  \tLoss: 75.6422 \n",
            "Epoch: 1/20  \tStep: 550  \tLoss: 74.3790 \n",
            "Epoch: 1/20  \tStep: 600  \tLoss: 73.4577 \n",
            "Epoch: 1/20  \tStep: 650  \tLoss: 76.4409 \n",
            "Epoch: 1/20  \tStep: 700  \tLoss: 79.8182 \n",
            "Epoch: 1/20  \tStep: 750  \tLoss: 73.9504 \n",
            "Epoch: 1/20  \tStep: 800  \tLoss: 78.0151 \n",
            "Epoch: 1/20  \tStep: 850  \tLoss: 77.3072 \n",
            "Epoch: 1/20  \tStep: 900  \tLoss: 79.7963 \n",
            "Epoch: 1/20  \tStep: 950  \tLoss: 73.5214 \n",
            "Epoch: 1/20  \tStep: 1000  \tLoss: 78.0971 \n",
            "Epoch: 1/20  \tStep: 1050  \tLoss: 78.6950 \n",
            "Epoch: 1/20  \tStep: 1100  \tLoss: 70.0106 \n",
            "Epoch: 1/20  \tStep: 1150  \tLoss: 75.6109 \n",
            "Epoch: 1/20  \tStep: 1200  \tLoss: 77.7426 \n",
            "Epoch: 1/20  \tStep: 1250  \tLoss: 78.8182 \n",
            "Epoch: 1/20  \tStep: 1300  \tLoss: 73.8083 \n",
            "Epoch: 1/20  \tStep: 1350  \tLoss: 71.8075 \n",
            "Epoch: 1/20  \tStep: 1400  \tLoss: 74.1046 \n",
            "Epoch: 1/20  \tStep: 1450  \tLoss: 74.9800 \n",
            "Epoch: 1/20  \tStep: 1500  \tLoss: 79.0907 \n",
            "Epoch: 1/20  \tStep: 1550  \tLoss: 72.0844 \n",
            "Epoch: 1/20  \tStep: 1600  \tLoss: 77.3509 \n",
            "Epoch: 1/20  \tStep: 1650  \tLoss: 71.7866 \n",
            "Epoch: 1/20  \tStep: 1700  \tLoss: 76.4830 \n",
            "Epoch: 1/20  \tStep: 1750  \tLoss: 72.4478 \n",
            "Epoch: 1/20  \tStep: 1800  \tLoss: 73.2614 \n",
            "Epoch: 1/20  \tStep: 1850  \tLoss: 72.4484 \n",
            "Epoch: 1/20  \tStep: 1900  \tLoss: 73.0647 \n",
            "Epoch: 1/20  \tStep: 1950  \tLoss: 74.4221 \n",
            "Epoch: 1/20  \tStep: 2000  \tLoss: 70.3840 \n",
            "Epoch: 1/20  \tStep: 2050  \tLoss: 75.1666 \n",
            "Epoch: 1/20  \tStep: 2100  \tLoss: 75.3273 \n",
            "Epoch: 1/20  \tStep: 2150  \tLoss: 68.8081 \n",
            "Epoch: 1/20  \tStep: 2200  \tLoss: 72.6406 \n",
            "Epoch: 1/20  \tStep: 2250  \tLoss: 70.5790 \n",
            "Epoch: 1/20  \tStep: 2300  \tLoss: 72.3262 \n",
            "Epoch: 1/20  \tStep: 2350  \tLoss: 72.0613 \n",
            "Epoch: 1/20  \tStep: 2400  \tLoss: 81.2374 \n",
            "Epoch: 1/20  \tStep: 2450  \tLoss: 72.6390 \n",
            "Epoch: 1/20  \tStep: 2500  \tLoss: 72.4759 \n",
            "Epoch: 1/20  \tStep: 2550  \tLoss: 71.7334 \n",
            "Epoch: 1/20  \tStep: 2600  \tLoss: 70.4178 \n",
            "Epoch: 1/20  \tStep: 2650  \tLoss: 75.2382 \n",
            "Epoch: 1/20  \tStep: 2700  \tLoss: 73.8735 \n",
            "Epoch: 1/20  \tStep: 2750  \tLoss: 73.1904 \n",
            "Epoch: 1/20  \tStep: 2800  \tLoss: 73.6680 \n",
            "Epoch: 1/20  \tStep: 2850  \tLoss: 76.3015 \n",
            "Epoch: 1/20  \tStep: 2900  \tLoss: 68.7601 \n",
            "Epoch: 1/20  \tStep: 2950  \tLoss: 72.4658 \n",
            "Epoch: 1/20  \tStep: 3000  \tLoss: 70.0407 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0llxEThJe1hD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjLJcMWVe1hO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}